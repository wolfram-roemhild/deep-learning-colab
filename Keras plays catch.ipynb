{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bd54c723c1d6335a43c8",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolfram-roemhild/deep-learning-colab/blob/master/Keras%20plays%20catch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XfS_p1hBbsTD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Somehow this model needs to be started with the last cell first. It does generate an error for now."
      ]
    },
    {
      "metadata": {
        "id": "ndaXB2b2ZL7H",
        "colab_type": "code",
        "outputId": "ccac981f-9c9b-4b61-e06a-42b45a264b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        }
      },
      "cell_type": "code",
      "source": [
        "train(model)\n",
        "test(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Stay, Points: 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAFKCAYAAABhFfaLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nweqi3pMD6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8beae3b208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a355d771d92e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-abca03a2deb3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mdisplay_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {:03d}/999 | Loss {:.4f} | Win count {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAFKCAYAAABhFfaLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nweqi3pMD6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8beae3b208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "PnhQ6q1gZL7M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.models import model_from_json\n",
        "#from qlearn import Catch\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "last_frame_time = 0\n",
        "translate_action = [\"Left\",\"Stay\",\"Right\",\"Create Ball\",\"End Test\"]\n",
        "grid_size = 10\n",
        "\n",
        "def display_screen(action,points,input_t):\n",
        "    global last_frame_time\n",
        "    display.clear_output(wait=True)\n",
        "    print (\"Action %s, Points: %d\" % (translate_action[action],points))\n",
        "    if(\"End\" not in translate_action[action]):\n",
        "        plt.imshow(input_t.reshape((grid_size,)*2),\n",
        "               interpolation='none', cmap='gray')\n",
        "        display.display(plt.gcf())\n",
        "    last_frame_time = set_max_fps(last_frame_time)\n",
        "def set_max_fps(last_frame_time,FPS = 1):\n",
        "    current_milli_time = lambda: int(round(time.time() * 1000))\n",
        "    sleep_time = 1./FPS - (current_milli_time() - last_frame_time)\n",
        "    if sleep_time > 0:\n",
        "        time.sleep(sleep_time)\n",
        "    return current_milli_time()\n",
        "def test(model):\n",
        "    global last_frame_time\n",
        "    plt.ion()\n",
        "    # Define environment, game\n",
        "    env = Catch(grid_size)\n",
        "    c = 0\n",
        "    last_frame_time = 0\n",
        "    points = 0\n",
        "    for e in range(10):\n",
        "        loss = 0.\n",
        "        env.reset()\n",
        "        game_over = False\n",
        "        # get initial input\n",
        "        input_t = env.observe()\n",
        "        display_screen(3,points,input_t)\n",
        "        c += 1\n",
        "        while not game_over:\n",
        "            input_tm1 = input_t\n",
        "            # get next action\n",
        "            q = model.predict(input_tm1)\n",
        "            action = np.argmax(q[0])\n",
        "            # apply action, get rewards and new state\n",
        "            input_t, reward, game_over = env.act(action)\n",
        "            points += reward\n",
        "            display_screen(action,points,input_t)\n",
        "            c += 1\n",
        "    display_screen(4,points,input_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCArN5mTZL7R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "acf122e0-deb3-4618-88c7-3042f51ce6ff"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import sgd\n",
        "\n",
        "\n",
        "class Catch(object):\n",
        "    def __init__(self, grid_size=10):\n",
        "        self.grid_size = grid_size\n",
        "        self.reset()\n",
        "\n",
        "    def _update_state(self, action):\n",
        "        \"\"\"\n",
        "        Input: action and states\n",
        "        Ouput: new states and reward\n",
        "        \"\"\"\n",
        "        state = self.state\n",
        "        if action == 0:  # left\n",
        "            action = -1\n",
        "        elif action == 1:  # stay\n",
        "            action = 0\n",
        "        else:\n",
        "            action = 1  # right\n",
        "        f0, f1, basket = state[0]\n",
        "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
        "        f0 += 1\n",
        "        out = np.asarray([f0, f1, new_basket])\n",
        "        out = out[np.newaxis]\n",
        "\n",
        "        assert len(out.shape) == 2\n",
        "        self.state = out\n",
        "\n",
        "    def _draw_state(self):\n",
        "        im_size = (self.grid_size,)*2\n",
        "        state = self.state[0]\n",
        "        canvas = np.zeros(im_size)\n",
        "        canvas[state[0], state[1]] = 1  # draw fruit\n",
        "        canvas[-1, state[2]-1:state[2] + 2] = 1  # draw basket\n",
        "        return canvas\n",
        "        \n",
        "    def _get_reward(self):\n",
        "        fruit_row, fruit_col, basket = self.state[0]\n",
        "        if fruit_row == self.grid_size-1:\n",
        "            if abs(fruit_col - basket) <= 1:\n",
        "                return 1\n",
        "            else:\n",
        "                return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def _is_over(self):\n",
        "        if self.state[0, 0] == self.grid_size-1:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self._draw_state()\n",
        "        return canvas.reshape((1, -1))\n",
        "\n",
        "    def act(self, action):\n",
        "        self._update_state(action)\n",
        "        reward = self._get_reward()\n",
        "        game_over = self._is_over()\n",
        "        return self.observe(), reward, game_over\n",
        "\n",
        "    def reset(self):\n",
        "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
        "        m = np.random.randint(1, self.grid_size-2, size=1)\n",
        "        self.state = np.asarray([0, n, m])[np.newaxis]\n",
        "\n",
        "\n",
        "class ExperienceReplay(object):\n",
        "    def __init__(self, max_memory=100, discount=.9):\n",
        "        self.max_memory = max_memory\n",
        "        self.memory = list()\n",
        "        self.discount = discount\n",
        "\n",
        "    def remember(self, states, game_over):\n",
        "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
        "        self.memory.append([states, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def get_batch(self, model, batch_size=10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_actions = model.output_shape[-1]\n",
        "        env_dim = self.memory[0][0][0].shape[1]\n",
        "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
        "        targets = np.zeros((inputs.shape[0], num_actions))\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
        "                                                  size=inputs.shape[0])):\n",
        "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "\n",
        "            inputs[i:i+1] = state_t\n",
        "            # There should be no target values for actions not taken.\n",
        "            # Thou shalt not correct actions not taken #deep\n",
        "            targets[i] = model.predict(state_t)[0]\n",
        "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
        "            if game_over:  # if game_over is True\n",
        "                targets[i, action_t] = reward_t\n",
        "            else:\n",
        "                # reward_t + gamma * max_a' Q(s', a')\n",
        "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
        "        return inputs, targets\n",
        "\n",
        "   \n",
        "# parameters\n",
        "epsilon = .1  # exploration\n",
        "num_actions = 3  # [move_left, stay, move_right]\n",
        "epoch = 1000\n",
        "max_memory = 500\n",
        "hidden_size = 100\n",
        "batch_size = 1\n",
        "grid_size = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
        "model.add(Dense(hidden_size, activation='relu'))\n",
        "model.add(Dense(num_actions))\n",
        "model.compile(sgd(lr=.2), \"mse\")\n",
        "    \n",
        "# If you want to continue training from a previous model, just uncomment the line bellow\n",
        "# model.load_weights(\"model.h5\")\n",
        "\n",
        "# Define environment/game\n",
        "env = Catch(grid_size)\n",
        "\n",
        "# Initialize experience replay object\n",
        "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
        "\n",
        "def train(model):\n",
        "    # Train\n",
        "    win_cnt = 0\n",
        "    for e in range(1):\n",
        "        loss = 0.\n",
        "        env.reset()\n",
        "        game_over = False\n",
        "        # get initial input\n",
        "        input_t = env.observe()\n",
        "\n",
        "        while not game_over:\n",
        "            input_tm1 = input_t\n",
        "            # get next action\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.randint(0, num_actions, size=1)\n",
        "            else:\n",
        "                q = model.predict(input_tm1)\n",
        "                action = np.argmax(q[0])\n",
        "\n",
        "            # apply action, get rewards and new state\n",
        "            input_t, reward, game_over = env.act(action)\n",
        "            if reward == 1:\n",
        "                win_cnt += 1\n",
        "\n",
        "            # store experience\n",
        "            exp_replay.remember([input_tm1, action, reward, input_t], game_over)            \n",
        "            \n",
        "            # adapt model\n",
        "            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
        "            \n",
        "            display_screen(action,3000,inputs[0])            \n",
        "            \n",
        "            loss += model.train_on_batch(inputs, targets)[0]\n",
        "        print(\"Epoch {:03d}/999 | Loss {:.4f} | Win count {}\".format(e, loss, win_cnt))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qc3X3einZL7V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ugb6vCSZL7Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}